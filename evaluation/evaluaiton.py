# í¬ë¡¤ë§ í•¨ìˆ˜
import requests
from bs4 import BeautifulSoup

def get_article_body(url:str, domain:str) -> str:
    # ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì‚¬ì´íŠ¸ë³„ íƒœê·¸ ì •ë³´#
    SITE_CLASS_MAPPING = {
        "bbc.com": [{"tag": "main", "class": "bbc-fa0wmp"}],
        "khan.co.kr": [{"tag": "div", "class": "art_body"}],
        "hani.co.kr": [{"tag": "div", "class": "article-text"}],
        "ytn.co.kr": [{"tag": "div", "class": "vodtext"}],
        "sisain.co.kr": [{"tag": "div", "class": "article-body"}],
        "news.sbs.co.kr": [{"tag": "div", "class": "main_text"}],
        "h21.hani.co.kr": [{"tag": "div", "class": "arti-txt"}],
        "ohmynews.com": [
            {"tag": "article", "class": "article_body at_contents article_view"},
            {"tag": "div", "class": "news_body"},
        ],
    }

    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36"}

    try:
        # URLì—ì„œ HTML (ìš”ì²­)ê°€ì ¸ì˜¤ê¸°
        response = requests.get(url, headers=headers)
        response.raise_for_status() # ì˜¤ë¥˜ ë°œìƒí•˜ë©´, ì˜ˆì™¸ ë°œìƒ
        soup = BeautifulSoup(response.text, "html.parser")

        # ë„ë©”ì¸ì´ ì œê³µëœ ê²½ìš° SITE_CLASS_MAPPINGì—ì„œ ì²˜ë¦¬
        site_info = SITE_CLASS_MAPPING.get(domain)
        if not site_info: # í•´ë‹¹ ë„ë©”ì¸ì— ëŒ€í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
            return None

        # ëª¨ë“  ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒí•˜ë©° íƒœê·¸/í´ë˜ìŠ¤ ì²˜ë¦¬
        for mapping in site_info:
            tag_name = mapping.get("tag")
            class_name = mapping.get("class")
            main_body = soup.find(tag_name, class_=class_name)
            if main_body:
                # íƒœê·¸ ë‚´ë¶€ì— p, h1 ë“±ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
                text_elements = main_body.find_all(["h1", "h2", "h3", "h4", "p", "li"])
                # <p> íƒœê·¸ ê°œìˆ˜ í™•ì¸ (2ê°œ ì´í•˜ì´ë©´ ë³¸ë¬¸ì´ ë¶€ì¡±í•˜ë‹¤ê³  ê°„ì£¼)

                paragraph_count = len(main_body.find_all("p"))
                if paragraph_count <= 2:
                    return main_body.get_text(strip=True)

                if text_elements:
                    return "\n".join(
                        [element.get_text(strip=True) for element in text_elements]
                    )
                else:
                    return main_body.get_text(strip=True)
        # í•´ë‹¹ ë„ë©”ì¸ì— ë§¤í•‘ëœ íƒœê·¸ì™€ í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        return None
    except requests.exceptions.RequestException as e: # HTTP ìš”ì²­ ì˜¤ë¥˜
        return None
    except Exception as e: # ë³¸ë¬¸ ì¶”ì¶œì¤‘ ì˜¤ë¥˜
        return None



# ëª¨ë¸ ë¡œë“œ
from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')


# 1. (ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ VS gpt ìƒì„± ì•„í‹°í´ ê²€ìƒ‰ í‚¤ì›Œë“œ)ê²€ì¦ í•¨ìˆ˜ 
def compute_similarities(user_input, keywords, label=""):
    emb_input = model.encode(user_input, convert_to_tensor=True)
    similarities = []

    print(f"\nğŸ”¹ ìœ ì‚¬ë„ ë¹„êµ: {label}")
    for kw in keywords:
        emb_kw = model.encode(kw, convert_to_tensor=True)
        sim = util.cos_sim(emb_input, emb_kw).item()
        similarities.append(sim)
        print(f"Input: '{user_input}' vs Keyword: '{kw}' => Similarity: {sim:.3f}")

    avg_sim = sum(similarities) / len(similarities)
    max_sim = max(similarities)

    print(f"ğŸ“Š Average similarity for '{label}': {avg_sim:.3f}")
    print(f"ğŸ”¥ Max similarity for '{label}': {max_sim:.3f}")

    return avg_sim, max_sim




# 2. (ì•„í‹°í´ ì¶”ì²œ ì •í™•ë„)ê²€ì¦ í•¨ìˆ˜
def compare_keyword_to_article(keywords: list, article_title: str, article_body: str, label: str = "") -> tuple:
    # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ â†’ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    keyword_text = " ".join(keywords)

    # ì„ë² ë”©
    emb_keyword = model.encode(keyword_text, convert_to_tensor=True)
    emb_title = model.encode(article_title, convert_to_tensor=True)
    emb_body = model.encode(article_body, convert_to_tensor=True)

    # ìœ ì‚¬ë„ ê³„ì‚°
    sim_title = util.cos_sim(emb_keyword, emb_title).item() # í‚¤ì›Œë“œ & ì œëª©
    sim_body = util.cos_sim(emb_keyword, emb_body).item()   # í‚¤ì›Œë“œ & ì•„í‹°í´

    # ì¶œë ¥
    print(f"\nğŸ” ìœ ì‚¬ë„ ë¹„êµ - {label}")
    print(f"Title similarity: {sim_title:.3f}")
    print(f"Body similarity: {sim_body:.3f}")

    return sim_title, sim_body



# ë°ì´í„° ë¡œë“œ (ì½”ë© ê¸°ì¤€)
from google.colab import drive
import pandas as pd

drive.mount('/content/drive')
file_path = "/content/drive/MyDrive/Colab Notebooks/capstone_test_data.xlsx"

input_df = pd.read_excel(file_path)
print(input_df.head())


# ê²€ì¦ ë°ì´í„° íŒŒì¼ ìƒì„±

results = []  # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸

for idx, row in input_df.iterrows():
    user_input1 = row['user_input1']
    user_input2 = row['user_input2']
    keyword1 = row['keyword1'].split(',')  # ë¬¸ìì—´ â†’ ë¦¬ìŠ¤íŠ¸
    keyword2 = row['keyword2'].split(',')
    url = row['URL']
    domain = row['Domain']
    title = row['title']
    speed = row['speed']

    # ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
    body = get_article_body(url, domain)

    # 1. ìœ ì € ì…ë ¥ vs GPT í‚¤ì›Œë“œ ìœ ì‚¬ë„
    avg1, max1 = compute_similarities(user_input1, keyword1)
    combined_input = user_input1 + " " + user_input2
    avg2, max2 = compute_similarities(combined_input, keyword2)

    # 2. í‚¤ì›Œë“œ vs ì•„í‹°í´(title/body)
    title_score, body_score = compare_keyword_to_article(keyword1, title, body)

    # 3. ê²°ê³¼ ì €ì¥
    results.append({
        "max1": max1,
        "avg1": avg1,
        "max2": max2,
        "avg2": avg2,
        "title_score": title_score,
        "body_score": body_score,
        "speed": speed
    })


results_df = pd.DataFrame(results)
results_df.to_excel("ê²€ì¦_ê²°ê³¼_ë¦¬í¬íŠ¸.xlsx", index=False) # âœ… ê²°ê³¼ DataFrame â†’ ì—‘ì…€ ì €ì¥


from google.colab import files
files.download("ê²€ì¦_ê²°ê³¼_ë¦¬í¬íŠ¸.xlsx")

print("âœ… ê²€ì¦ ì™„ë£Œ: 'ê²€ì¦_ê²°ê³¼_ë¦¬í¬íŠ¸.xlsx'ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
